{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc329a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0 — imports; config; logging\n",
    "import requests\n",
    "import os, json, time, hashlib, warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from beir import util as beir_util\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80920a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters \n",
    "DATASET_SPEC = \"beir:fiqa\" \n",
    "CACHE_DIR = \"./cache\"\n",
    "REPORT_DIR = \"./reports\"\n",
    "K_VALUES = [1, 3, 10]\n",
    "\n",
    "# LLM-judge parameters\n",
    "JUDGE_ENABLED   = True\n",
    "OLLAMA_URL      = \"http://localhost:11434/api/generate\"\n",
    "JUDGE_MODEL     = \"llama3.1:8b-instruct-q8_0\" \n",
    "JUDGE_TOP_K     = 5                        \n",
    "JUDGE_MAX_Q     = 10   \n",
    "\n",
    "# Retrievers\n",
    "MODEL_REGISTRY = [\n",
    "    {\"name\": \"DPR\",   \"hf_id\": \"facebook-dpr-question_encoder-single-nq-base\",      \"type\": \"dense\"},\n",
    "    {\"name\": \"E5\",    \"hf_id\": \"intfloat/e5-base-v2\",                               \"type\": \"dense\"},\n",
    "    {\"name\": \"MPNet\", \"hf_id\": \"sentence-transformers/multi-qa-mpnet-base-dot-v1\",  \"type\": \"dense\"},\n",
    "]\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(REPORT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8dd12ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57638/57638 [00:00<00:00, 219600.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset → docs=200; queries=7 (all with labels)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Data loader\n",
    "def load_beir_dataset(spec: str, base_dir=\"./datasets\"):\n",
    "    assert spec.startswith(\"beir:\"), \"This cell handles BEIR only; see custom loader later.\"\n",
    "    name = spec.split(\":\", 1)[1]\n",
    "    url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{name}.zip\"\n",
    "    data_path = beir_util.download_and_unzip(url, base_dir)          # -> ./datasets/<name>\n",
    "    corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
    "    return data_path, corpus, queries, qrels\n",
    "\n",
    "def normalize_corpus_texts(corpus: dict):\n",
    "    \n",
    "    ids, texts = [], []\n",
    "    for did, obj in corpus.items():\n",
    "        t = (obj.get(\"title\") or \"\").strip()\n",
    "        x = (obj.get(\"text\") or \"\").strip()\n",
    "        txt = (t + \". \" + x).strip() if t and x else (t or x)\n",
    "        ids.append(did); texts.append(txt)\n",
    "    return ids, texts\n",
    "\n",
    "DATA_PATH, CORPUS, QUERIES, QRELS = load_beir_dataset(DATASET_SPEC)\n",
    "DOC_IDS, DOC_TEXTS = normalize_corpus_texts(CORPUS)\n",
    "\n",
    "# Subset sizes\n",
    "N_DOCS = 200     \n",
    "N_QUERIES = 10 \n",
    "K_VALUES = [1, 3, 10]\n",
    "\n",
    "\n",
    "doc_ids_all = list(CORPUS.keys())\n",
    "doc_ids_sub = set(doc_ids_all[:N_DOCS])\n",
    "corpus_sub = {did: CORPUS[did] for did in doc_ids_all[:N_DOCS]}\n",
    "\n",
    "qrels_sub = {}\n",
    "for qid, drels in QRELS.items():\n",
    "    keep = {did: rel for did, rel in drels.items() if did in doc_ids_sub}\n",
    "    if keep:\n",
    "        qrels_sub[qid] = keep\n",
    "\n",
    "query_ids_labeled = list(qrels_sub.keys())[:N_QUERIES]\n",
    "queries_sub = {qid: QUERIES[qid] for qid in query_ids_labeled}\n",
    "qrels_sub = {qid: qrels_sub[qid] for qid in query_ids_labeled}\n",
    "\n",
    "# encoding\n",
    "doc_texts_sub = []\n",
    "doc_ids_seq = []\n",
    "for did, obj in corpus_sub.items():\n",
    "    t = (obj.get(\"title\") or \"\").strip()\n",
    "    x = (obj.get(\"text\") or \"\").strip()\n",
    "    doc_texts_sub.append((t + \". \" + x).strip() if t and x else (t or x))\n",
    "    doc_ids_seq.append(did)\n",
    "\n",
    "print(f\"Subset → docs={len(doc_ids_seq)}; queries={len(queries_sub)} (all with labels)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b47ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def model_key(hf_id: str) -> str:\n",
    "    return hashlib.md5(hf_id.encode()).hexdigest()[:10]\n",
    "\n",
    "def load_st_model(hf_id: str):\n",
    "    return SentenceTransformer(hf_id, trust_remote_code=True)\n",
    "\n",
    "def cache_paths(cache_dir, dataset_path, model_id):\n",
    "    base = os.path.join(cache_dir, f\"{os.path.basename(dataset_path)}__{model_id}\")\n",
    "    return base + \".docs.npy\", base + \".meta.json\"\n",
    "\n",
    "def encode_docs_cached(model, texts, cache_dir, dataset_path, model_id):\n",
    "    emb_path, meta_path = cache_paths(cache_dir, dataset_path, model_id)\n",
    "    if os.path.exists(emb_path):\n",
    "        emb_docs = np.load(emb_path)\n",
    "        return emb_docs, {\"cached\": True}\n",
    "    t0 = time.time()\n",
    "    emb_docs = model.encode(texts, normalize_embeddings=True, show_progress_bar=False)\n",
    "    np.save(emb_path, emb_docs)\n",
    "    meta = {\"cached\": False, \"encode_secs\": round(time.time() - t0, 3), \"n_docs\": len(texts)}\n",
    "    with open(meta_path, \"w\") as f: json.dump(meta, f)\n",
    "    return emb_docs, meta\n",
    "\n",
    "def evaluate_model(model, doc_ids, emb_docs, queries, qrels, k_values):\n",
    "    results = {}\n",
    "    latencies = []\n",
    "    for qid, q in queries.items():\n",
    "        t0 = time.time()\n",
    "        qvec = model.encode(q, normalize_embeddings=True)\n",
    "        scores = emb_docs @ qvec\n",
    "        latencies.append(time.time() - t0)\n",
    "        results[qid] = {doc_ids[i]: float(scores[i]) for i in range(len(doc_ids))}\n",
    "    retr = EvaluateRetrieval(\"cos_sim\")\n",
    "    ndcg, _map, recall, precision = retr.evaluate(qrels, results, k_values=k_values)\n",
    "    return {\n",
    "        \"nDCG\": ndcg,\n",
    "        \"MAP\": _map,\n",
    "        \"Recall\": recall,\n",
    "        \"Precision\": precision,\n",
    "        \"latency_avg_ms\": round(1000 * (sum(latencies) / max(1, len(latencies))), 2)\n",
    "    }\n",
    "\n",
    "def recommend(models_metrics: dict, primary=\"NDCG@10\"):\n",
    "    # rank by quality first, break ties by coverage then latency. suject to change\n",
    "    def key(m):\n",
    "        s = models_metrics[m]\n",
    "        return (\n",
    "            s[\"nDCG\"].get(primary, 0.0),\n",
    "            s[\"Recall\"].get(\"Recall@10\", 0.0),\n",
    "            -s[\"latency_avg_ms\"]\n",
    "        )\n",
    "    return sorted(models_metrics.keys(), key=key, reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56bb4473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text map for judge and printing\n",
    "DOC_TEXT_BY_ID = dict(zip(doc_ids_seq, doc_texts_sub))\n",
    "\n",
    "def rank_topk(model, emb_docs, doc_ids, queries, topk=5):\n",
    "    ranked = {}\n",
    "    for qid, q in queries.items():\n",
    "        qvec = model.encode(q, normalize_embeddings=True)\n",
    "        scores = emb_docs @ qvec\n",
    "        order = np.argsort(scores)[::-1][:topk]\n",
    "        ranked[qid] = [(doc_ids[i], float(scores[i])) for i in order]\n",
    "    return ranked\n",
    "\n",
    "# ---- LLM judge ----\n",
    "_JUDGE_RUBRIC = \"\"\"You are an impartial retrieval judge for financial Q&A.\n",
    "For each user query and the candidate passages, assign integer scores 0-5 for:\n",
    "- relevance (semantic match),\n",
    "- specificity (directness to the question),\n",
    "- faithfulness (no contradiction within the snippet; ignore outside knowledge),\n",
    "- overall (holistic usefulness; not an average).\n",
    "Return STRICT JSON with schema:\n",
    "{\n",
    "  \"per_passage\":[\n",
    "    {\"doc_id\":\"<id>\",\"relevance\":0,\"specificity\":0,\"faithfulness\":0,\"overall\":0,\"justification\":\"<=350 chars\"}\n",
    "  ],\n",
    "  \"best_doc_id\":\"<id>\",\n",
    "  \"reason_best\":\"<=200 chars\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def _ollama_generate(prompt: str, model: str = JUDGE_MODEL, url: str = OLLAMA_URL, max_retries: int = 3):\n",
    "    payload = {\"model\": model, \"prompt\": prompt, \"options\": {\"temperature\": 0.2}, \"stream\": False}\n",
    "    last = None\n",
    "    for a in range(max_retries):\n",
    "        r = requests.post(url, json=payload, timeout=120)\n",
    "        last = r\n",
    "        if r.ok:\n",
    "            return r.json()[\"response\"]\n",
    "        time.sleep(1.2 * (a + 1))\n",
    "    raise RuntimeError(f\"Ollama request failed: {getattr(last,'text','<no response>')}\")\n",
    "\n",
    "def _safe_score(x):\n",
    "    try:\n",
    "        v = float(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    return max(1.0, min(5.0, v))\n",
    "\n",
    "def judge_query(query_text: str, ranked_docs: list) -> dict:\n",
    "    cands = []\n",
    "    for did, _ in ranked_docs:\n",
    "        txt = (DOC_TEXT_BY_ID.get(did, \"\") or \"\").replace(\"\\n\", \" \").strip()\n",
    "        cands.append(f\"[{did}] {txt[:900]}\")\n",
    "    prompt = (\n",
    "        f\"Query:\\n{query_text}\\n\\n\"\n",
    "        f\"Candidate passages:\\n\" + \"\\n\\n\".join(cands) + \"\\n\\n\"\n",
    "        f\"Instructions:\\n\" + _JUDGE_RUBRIC\n",
    "    )\n",
    "\n",
    "    raw = _ollama_generate(prompt)\n",
    "    raw = raw.strip().strip(\"`\")\n",
    "    try:\n",
    "        obj = json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        i, j = raw.find(\"{\"), raw.rfind(\"}\")\n",
    "        obj = json.loads(raw[i:j+1])\n",
    "\n",
    "    # --- normalize/clamp all numeric fields to [1,5] ---\n",
    "    if \"per_passage\" in obj and isinstance(obj[\"per_passage\"], list):\n",
    "        norm = []\n",
    "        for p in obj[\"per_passage\"]:\n",
    "            p = dict(p)  # copy\n",
    "            p[\"relevance\"]   = _safe_score(p.get(\"relevance\", 0))\n",
    "            p[\"specificity\"] = _safe_score(p.get(\"specificity\", 0))\n",
    "            p[\"faithfulness\"]= _safe_score(p.get(\"faithfulness\", 0))\n",
    "            p[\"overall\"]     = _safe_score(p.get(\"overall\", 0))\n",
    "            norm.append(p)\n",
    "        obj[\"per_passage\"] = norm\n",
    "    return obj\n",
    "\n",
    "\n",
    "def summarize_judge(per_model_rankings: dict, queries: dict, max_q: int, topk: int):\n",
    "    \"\"\"\n",
    "    per_model_rankings: {model_name: {qid: [(doc_id, score), ...]}}\n",
    "    returns: (df_per_query, df_summary, pairwise_wins_matrix)\n",
    "    \"\"\"\n",
    "    qids = list(queries.keys())[:max_q]\n",
    "    rows = []\n",
    "    models = list(per_model_rankings.keys())\n",
    "    wins = {m: {n: 0 for n in models} for m in models}\n",
    "\n",
    "    for qid in qids:\n",
    "        avgs = {}\n",
    "        for m in models:\n",
    "            judged = judge_query(queries[qid], per_model_rankings[m][qid][:topk])\n",
    "            scores = [ _safe_score(p.get(\"overall\", 0)) for p in judged[\"per_passage\"] ]\n",
    "            scores = [s for s in scores if not np.isnan(s)]\n",
    "            overall = float(np.mean(scores)) if scores else np.nan\n",
    "\n",
    "            avgs[m] = overall\n",
    "            rows.append({\"qid\": qid, \"model\": m, \"overall_avg_topK\": overall,\n",
    "                         \"best_doc_id\": judged.get(\"best_doc_id\", \"\")})\n",
    "        best = max(avgs.values())\n",
    "        winners = [m for m,v in avgs.items() if np.isclose(v, best)]\n",
    "        for w in winners:\n",
    "            for o in models:\n",
    "                if o != w:\n",
    "                    wins[w][o] += 1\n",
    "\n",
    "    df_perq = pd.DataFrame(rows)\n",
    "    df_summary = df_perq.groupby(\"model\")[\"overall_avg_topK\"].mean().sort_values(ascending=False).to_frame(\"JudgeOverall@topK\")\n",
    "    win_mat = pd.DataFrame(wins).T\n",
    "    return df_perq, df_summary, win_mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743cb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating DPR …\n",
      "Evaluating E5 …\n",
      "Evaluating MPNet …\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "nDCG@1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nDCG@3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nDCG@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Recall@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "P@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Avg Latency (ms)",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "b879a4be-e439-4428-b321-43019feddfaf",
       "rows": [
        [
         "0",
         "E5",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "0.1",
         "28.91"
        ],
        [
         "1",
         "MPNet",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "0.1",
         "28.08"
        ],
        [
         "2",
         "DPR",
         "0.5714",
         "0.7517",
         "0.7947",
         "1.0",
         "0.1",
         "31.92"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>nDCG@1</th>\n",
       "      <th>nDCG@3</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>P@10</th>\n",
       "      <th>Avg Latency (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E5</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>28.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MPNet</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>28.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DPR</td>\n",
       "      <td>0.5714</td>\n",
       "      <td>0.7517</td>\n",
       "      <td>0.7947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>31.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model  nDCG@1  nDCG@3  nDCG@10  Recall@10  P@10  Avg Latency (ms)\n",
       "0     E5  1.0000  1.0000   1.0000        1.0   0.1             28.91\n",
       "1  MPNet  1.0000  1.0000   1.0000        1.0   0.1             28.08\n",
       "2    DPR  0.5714  0.7517   0.7947        1.0   0.1             31.92"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation loop\n",
    "all_metrics = {}\n",
    "ranked_per_model = {}\n",
    "\n",
    "for m in MODEL_REGISTRY:\n",
    "    name, hf_id = m[\"name\"], m[\"hf_id\"]\n",
    "    print(f\"Evaluating {name} …\")\n",
    "    model = load_st_model(hf_id)\n",
    "    emb_docs, meta = encode_docs_cached(model, doc_texts_sub, CACHE_DIR, DATA_PATH, model_key(hf_id))\n",
    "    metrics = evaluate_model(model, doc_ids_seq, emb_docs, queries_sub, qrels_sub, k_values=K_VALUES)\n",
    "    all_metrics[name] = metrics\n",
    "\n",
    "    ranked_per_model[name] = rank_topk(model, emb_docs, doc_ids_seq, queries_sub, topk=JUDGE_TOP_K)\n",
    "\n",
    "# Tabular summary\n",
    "rows = []\n",
    "for name, m in all_metrics.items():\n",
    "    row = {\n",
    "        \"Model\": name,\n",
    "        \"nDCG@1\": round(m[\"nDCG\"].get(\"NDCG@1\", 0.0), 4),\n",
    "        \"nDCG@3\": round(m[\"nDCG\"].get(\"NDCG@3\", 0.0), 4),\n",
    "        \"nDCG@10\": round(m[\"nDCG\"].get(\"NDCG@10\", 0.0), 4),\n",
    "        \"Recall@10\": round(m[\"Recall\"].get(\"Recall@10\", 0.0), 4),\n",
    "        \"P@10\": round(m[\"Precision\"].get(\"P@10\", 0.0), 4),\n",
    "        \"Avg Latency (ms)\": m[\"latency_avg_ms\"]\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values(\"nDCG@10\", ascending=False).reset_index(drop=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d279bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLM-judge …\n",
      "\n",
      "LLM-Judge summary (higher is better):\n",
      "       JudgeOverall@topK\n",
      "model                   \n",
      "MPNet              3.800\n",
      "E5                 3.543\n",
      "DPR                3.429\n",
      "\n",
      "Pairwise wins (queries judged):\n",
      "       DPR  E5  MPNet\n",
      "DPR      0   3      3\n",
      "E5       3   0      3\n",
      "MPNet    5   5      0\n"
     ]
    }
   ],
   "source": [
    "if JUDGE_ENABLED:\n",
    "    print(\"Running LLM-judge …\")\n",
    "    df_judge_perq, df_judge_summary, df_winmat = summarize_judge(\n",
    "        ranked_per_model, queries_sub, max_q=JUDGE_MAX_Q, topk=JUDGE_TOP_K\n",
    "    )\n",
    "    \n",
    "    print(\"\\nLLM-Judge summary (higher is better):\")\n",
    "    print(df_judge_summary.round(3))\n",
    "    print(\"\\nPairwise wins (queries judged):\")\n",
    "    print(df_winmat)\n",
    "else:\n",
    "    df_judge_perq = df_judge_summary = df_winmat = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "025f76d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation (BEIR): MPNet > E5 > DPR\n",
      "Judge ranking: MPNet > E5 > DPR\n",
      "Report: ./reports\\fiqa_report.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "nDCG@1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nDCG@3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nDCG@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Recall@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "P@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Avg Latency (ms)",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "91ecced1-6929-4fc1-9851-08ab4e6bebe0",
       "rows": [
        [
         "0",
         "E5",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "0.1",
         "28.91"
        ],
        [
         "1",
         "MPNet",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "0.1",
         "28.08"
        ],
        [
         "2",
         "DPR",
         "0.5714",
         "0.7517",
         "0.7947",
         "1.0",
         "0.1",
         "31.92"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>nDCG@1</th>\n",
       "      <th>nDCG@3</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>P@10</th>\n",
       "      <th>Avg Latency (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E5</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>28.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MPNet</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>28.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DPR</td>\n",
       "      <td>0.5714</td>\n",
       "      <td>0.7517</td>\n",
       "      <td>0.7947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>31.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model  nDCG@1  nDCG@3  nDCG@10  Recall@10  P@10  Avg Latency (ms)\n",
       "0     E5  1.0000  1.0000   1.0000        1.0   0.1             28.91\n",
       "1  MPNet  1.0000  1.0000   1.0000        1.0   0.1             28.08\n",
       "2    DPR  0.5714  0.7517   0.7947        1.0   0.1             31.92"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = recommend(all_metrics, primary=\"NDCG@10\")\n",
    "\n",
    "report = {\n",
    "    \"dataset\": DATASET_SPEC,\n",
    "    \"models\": MODEL_REGISTRY,\n",
    "    \"k_values\": K_VALUES,\n",
    "    \"metrics\": all_metrics,\n",
    "    \"recommendation_order\": order,\n",
    "    \"llm_judge\": {\n",
    "        \"enabled\": bool(JUDGE_ENABLED),\n",
    "        \"model\": JUDGE_MODEL if JUDGE_ENABLED else None,\n",
    "        \"top_k\": JUDGE_TOP_K,\n",
    "        \"max_queries\": JUDGE_MAX_Q,\n",
    "        \"per_query\": (df_judge_perq.to_dict(orient=\"records\") if df_judge_perq is not None else None),\n",
    "        \"summary\": (df_judge_summary[\"JudgeOverall@topK\"].to_dict() if df_judge_summary is not None else None),\n",
    "        \"pairwise_wins\": (df_winmat.to_dict() if df_winmat is not None else None)\n",
    "    }\n",
    "}\n",
    "out_path = os.path.join(REPORT_DIR, \"fiqa_report.json\")\n",
    "with open(out_path, \"w\") as f: json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"Recommendation (BEIR):\", \" > \".join(order))\n",
    "if df_judge_summary is not None:\n",
    "    print(\"Judge ranking:\", \" > \".join(list(df_judge_summary.index)))\n",
    "print(\"Report:\", out_path)\n",
    "df\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
